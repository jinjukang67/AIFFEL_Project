{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GD_Transformer_translator_project.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNuM5C4ZFv/2yX3R7pN6/jc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinjukang67/AIFFEL_Project/blob/master/GD_Transformer_translator_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 프로젝트 디렉토리 생성\n",
        "#!mkdir -p /content/drive/MyDrive/Aiffel/transformer"
      ],
      "metadata": {
        "id": "DJDxuyLIMbJZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "# 한글 실행 위해서 \n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        " \n",
        "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
        "font = fm.FontProperties(fname=fontpath, size=9)\n",
        "plt.rc('font', family='NanumBarunGothic') \n",
        "mpl.font_manager.findfont(font)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "6jKVjjhqNxAm",
        "outputId": "02b2f232-e3b4-472b-e367-8199da97b15f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import re\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import random\n",
        "\n",
        "import seaborn # Attention 시각화를 위해 필요!\n",
        "\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_mNSUgTN7ek",
        "outputId": "14ff85f3-6734-4b0b-c8d6-b66dbefbe4ae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(pos, d_model):\n",
        "    def cal_angle(position, i):\n",
        "        return position / np.power(10000, int(i) / d_model)\n",
        "\n",
        "    def get_posi_angle_vec(position):\n",
        "        return [cal_angle(position, i) for i in range(d_model)]\n",
        "\n",
        "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
        "    return sinusoid_table"
      ],
      "metadata": {
        "id": "tqC75cIuN-qb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        self.depth = d_model // self.num_heads\n",
        "        \n",
        "        self.W_q = tf.keras.layers.Dense(d_model) # Linear Layer\n",
        "        self.W_k = tf.keras.layers.Dense(d_model)\n",
        "        self.W_v = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "        self.linear = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
        "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
        "\n",
        "        \"\"\"\n",
        "        Scaled QK 값 구하기\n",
        "        \"\"\"\n",
        "\n",
        "        if mask is not None: scaled_qk += (mask * -1e9) \n",
        "\n",
        "        \"\"\"\n",
        "        1. Attention Weights 값 구하기 -> attentions\n",
        "        2. Attention 값을 V에 곱하기 -> out\n",
        "        \"\"\" \n",
        "\n",
        "        return out, attentions\n",
        "        \n",
        "\n",
        "    def split_heads(self, x):\n",
        "        \"\"\"\n",
        "        Embedding을 Head의 수로 분할하는 함수\n",
        "\n",
        "        x: [ batch x length x emb ]\n",
        "        return: [ batch x length x heads x self.depth ]\n",
        "        \"\"\"\n",
        "\n",
        "        return split_x\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        \"\"\"\n",
        "        분할된 Embedding을 하나로 결합하는 함수\n",
        "\n",
        "        x: [ batch x length x heads x self.depth ]\n",
        "        return: [ batch x length x emb ]\n",
        "        \"\"\"\n",
        "\n",
        "        return combined_x\n",
        "    \n",
        "\n",
        "    def call(self, Q, K, V, mask):\n",
        "        \"\"\"\n",
        "        아래 순서에 따라 소스를 작성하세요.\n",
        "\n",
        "        Step 1: Linear_in(Q, K, V) -> WQ, WK, WV\n",
        "        Step 2: Split Heads(WQ, WK, WV) -> WQ_split, WK_split, WV_split\n",
        "        Step 3: Scaled Dot Product Attention(WQ_split, WK_split, WV_split)\n",
        "                 -> out, attention_weights\n",
        "        Step 4: Combine Heads(out) -> out\n",
        "        Step 5: Linear_out(out) -> out\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        return out, attention_weights"
      ],
      "metadata": {
        "id": "TH41-QymOA46"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_2DGoD81OCSu"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}