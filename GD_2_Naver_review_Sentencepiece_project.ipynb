{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GD_2_Naver_review_Sentencepiece_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPx5Ns1ZCFYbybVV1XmHCbN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinjukang67/AIFFEL_Project/blob/master/GD_2_Naver_review_Sentencepiece_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 네이버 영화 리뷰 감성 분석 프로젝트 : SentencePiece 적용해 보기"
      ],
      "metadata": {
        "id": "-O_ojSov-cFv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1. SentencePiece 설치하기\n",
        "---"
      ],
      "metadata": {
        "id": "ZqrRx_F7_Djs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QOHaIiKs_UQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n",
        "!pip install wget"
      ],
      "metadata": {
        "id": "LWP6_six-qBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2. SentencePiece 모델 학습"
      ],
      "metadata": {
        "id": "hVQ0YyFcDQwo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) 데이터 준비하기\n",
        "터미널로 데이터 폴더 생성하기\n",
        "```\n",
        "! bash\n",
        "cd /drive/MyDrive/Aiffel\n",
        "mkdir -p sp_tokenizer/data\n",
        "exit\n",
        "```"
      ],
      "metadata": {
        "id": "DfnbhqgvCE0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !bash"
      ],
      "metadata": {
        "id": "1WjKhGhABSwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data를 저장할 폴더 경로\n",
        "data_dir = \"/content/drive/MyDrive/Aiffel/sp_tokenizer/data\""
      ],
      "metadata": {
        "id": "rJlxEQDJ_9lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 Import\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "import wget\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import urllib.request\n",
        "import csv\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "_DFcKo3NCs5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filename = wget.download(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", f\"{data_dir}\")\n",
        "# print(filename)\n",
        "# filename = wget.download(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", f\"{data_dir}\")\n",
        "# print(filename)\n",
        "for f in os.listdir(data_dir):\n",
        "  print(f)"
      ],
      "metadata": {
        "id": "KYmTZ7a4Cwou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Korpora가 뭔지 알아보기\n",
        "> - **Korpora(한국어 코퍼스 데이터 오픈소스 파이썬 패키지)** 한번 사용해보기\n",
        "> - Sentence Piece 사용하자\n",
        "\n",
        "[Korpora 깃헙](https://github.com/ko-nlp/Korpora)<br>\n",
        "[Korpora 빠른 사용법](https://ko-nlp.github.io/Korpora/ko-docs/introduction/quicktour.html)"
      ],
      "metadata": {
        "id": "lnXXrTpaEo5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git"
      ],
      "metadata": {
        "id": "IiyZF6I3Jjmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Korpora"
      ],
      "metadata": {
        "id": "Vi55qMBdJhoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Korpora import Korpora\n",
        "\n",
        "# Korpora.fetch(\"all\") -> Korpora가 제공하는 모든 말뭉치 내려 받을 수 있음\n",
        "Korpora.fetch(\"nsmc\") # 네이버 영화 리뷰 데이터로만 일단 단어장 사용해보자!\n",
        "corpus = Korpora.load(\"nsmc\")"
      ],
      "metadata": {
        "id": "-6Lu3rltJwsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "id": "abF3iSelLEua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> - **Korpora**는 그냥 다양한 한국어 코퍼스 데이터를 쉽게 다운로드, 사용할 수만 있는 기능만 제공\n",
        "> - 터미널에서 전체 데이터로 다운로드 전처리 하려면\n",
        "```\n",
        "korpora lmdata \\\n",
        "  --corpus all \\\n",
        "  --output_dir ~/works/lmdata\n",
        "```\n",
        "\n",
        "> - 네이버 영화 리뷰 데이터는 이미 위에서 준비 완료했으므로 1회 실습 후 더 큰 데이터를 **Korpora**에서 불러와 Vocab 만들어 실습 진행해보자"
      ],
      "metadata": {
        "id": "D8lIrnO6LR5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) 데이터 정제하기"
      ],
      "metadata": {
        "id": "8tbi-jsGKsZy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`rating_train_txt` document만 뽑아기\n",
        "> - 데이터프레임으로 바꿔서 csv 파일로 저장한 후 \n",
        "> - 다시 txt 파일 형태로 바꿔서 저장하자.\n",
        "\n",
        "=> 오류나서 실패^^ <br>\n",
        "=> 그냥 for문으로 split() 써서 가져왔는데 더 간단한 방법이 있었다."
      ],
      "metadata": {
        "id": "cTVEywUrOQik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이렇게 쉽게 가져오는 방법도 있었다^^\n",
        "```\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\n",
        "\n",
        "with open('naver_review.txt', 'w', encoding='utf8') as f:\n",
        "    f.write('\\n'.join(naver_df['document']))\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "odXg7OESbS0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# temp = pd.read_table(path_to_file)\n",
        "# temp.head()\n",
        "\n",
        "# temp.to_csv(data_dir + '/ratings_train.csv', index=False, header=True)"
      ],
      "metadata": {
        "id": "cWC4ZErtCONG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# csv 파일d text로 변환 후 document만 가져오기 - 실패\n",
        "'''\n",
        "import shutil\n",
        "\n",
        "in_file = f\"{data_dir}/ratings_train.csv\"\n",
        "out_file = f\"{data_dir}/ratings_train_f.txt\"\n",
        "SEPARATOR = u\"\\u241D\"\n",
        "\n",
        "if not os.path.isfile(out_file):\n",
        "    df = pd.read_csv(in_file, sep=SEPARATOR, engine=\"python\")\n",
        "    with open(out_file, \"w\") as f:\n",
        "      for index, row in df.iterrows():\n",
        "        f.write(row[\"document\"]) # document만 가져오기\n",
        "        f.write(\"\\n\\n\\n\\n\") # 구분자\n",
        "\n",
        "shutil.copy(out_file, \"ratings_train_f.txt\")\n",
        "'''"
      ],
      "metadata": {
        "id": "VQAX46-NPBt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = data_dir + \"/ratings_train.txt\"\n",
        "\n",
        "with open(path_to_file, \"r\") as f:\n",
        "    raw = f.read().splitlines()\n",
        "\n",
        "print(\"Data Size:\", len(raw))\n",
        "\n",
        "print(\"Example:\")\n",
        "for sen in raw[1:100][::20]: print(\">>\", sen.split('\t')[1])"
      ],
      "metadata": {
        "id": "9CwC7N4rN5lK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train 데이터\n",
        "ratings = [] \n",
        "for sen in raw[1:]: \n",
        "    ratings.append(sen.split('\t')[1])"
      ],
      "metadata": {
        "id": "3d30KRtZmJTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 라벨 데이터 y_train\n",
        "y_train = []\n",
        "for sen in raw[1:]:\n",
        "    y_train.append(int(sen.split('\t')[2]))\n",
        "\n",
        "y_train[:20]"
      ],
      "metadata": {
        "id": "psYdaqRfBsde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(ratings)"
      ],
      "metadata": {
        "id": "15xM5f-krIaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test 데이터셋 가져오기 1\n",
        "test_path = data_dir + \"/ratings_test.txt\"\n",
        "X_test = []\n",
        "with open(test_path, \"r\") as f:\n",
        "    raw_t = f.read().splitlines()\n",
        "\n",
        "for sen in raw_t[1:]:\n",
        "    X_test.append(sen.split('\t')[1])\n",
        "\n",
        "y_test = []\n",
        "for sen in raw_t[1:]:\n",
        "    y_test.append(int(sen.split('\t')[2]))\n",
        "\n",
        "print(\"X_test data Size:\", len(X_test))\n",
        "print(\"y_test data Size:\", len(y_test))\n",
        "print(X_test[:10])\n",
        "print(y_test[:10])"
      ],
      "metadata": {
        "id": "JV2QGQ0C6qYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test 데이터셋 가져오기 2\n",
        "test_path = data_dir + \"/ratings_test.txt\"\n",
        "\n",
        "test_data =pd.read_table(test_path)\n",
        "test_data = test_data.dropna(how = 'any')\n",
        "test_data.drop_duplicates(subset=['document'],inplace=True)\n",
        "\n",
        "print(test_data.isnull().values.any()) # Null 값이 존재하는지 확인\n",
        "print(test_data['document'].nunique())\n",
        "print(len(test_data))"
      ],
      "metadata": {
        "id": "5TFPIe81cuOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 문장 길이 분포 시각화\n",
        "min_len = 999\n",
        "max_len = 0\n",
        "sum_len = 0\n",
        "\n",
        "for sen in ratings:\n",
        "    length = len(sen)\n",
        "    if min_len > length: min_len = length\n",
        "    if max_len < length: max_len = length\n",
        "    sum_len += length\n",
        "\n",
        "print(\"문장의 최단 길이:\", min_len)\n",
        "print(\"문장의 최장 길이:\", max_len)\n",
        "print(\"문장의 평균 길이:\", sum_len // len(raw))\n",
        "\n",
        "sentence_length = np.zeros((max_len), dtype=np.int)\n",
        "\n",
        "print(sentence_length)\n",
        "\n",
        "for sen in ratings:\n",
        "    sentence_length[len(sen)-1] += 1 # 문장 길이 빈도수 표현\n",
        "\n",
        "print(sentence_length)\n",
        "plt.bar(range(max_len), sentence_length, width=1.0)\n",
        "plt.title(\"Sentence Length Distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bx8_JfnIPLiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) 최단 길이가 0이 왜 나와?\n",
        "def check_sentence_with_length(raw, length):\n",
        "    count = 0\n",
        "\n",
        "    for sen in raw:\n",
        "        if len(sen) == length:\n",
        "            print(sen)\n",
        "            count += 1\n",
        "            if count > 100: return\n",
        "\n",
        "# check_sentence_with_length(ratings, 1)"
      ],
      "metadata": {
        "id": "xXwkYiAHimjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_sentence_with_length(ratings, 2)"
      ],
      "metadata": {
        "id": "N__QY3Eqrvs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check_sentence_with_length(ratings, 60)"
      ],
      "metadata": {
        "id": "xl_yivwttxhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> - 최단길이가 0인걸 봐서는 공백이 들어간거 같다, 공백은 제거하자\n",
        "> - 굿,굳과 같은 단어는 긍정의 의미 담고 있으니까 길이 1이라도 없애지 말자."
      ],
      "metadata": {
        "id": "mmpcOiXao7Jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장의 길이가 같은 문장이 2000개 초과하는 sentence_length는 뭘까?\n",
        "for idx, _sum in enumerate(sentence_length):\n",
        "    if _sum > 3000:\n",
        "        print(\"Outlier Index: \",idx+1)"
      ],
      "metadata": {
        "id": "t38wVNYHjxD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check_sentence_with_length(ratings, 24)"
      ],
      "metadata": {
        "id": "ik1XuxWvp4Ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 중복 있을 수도 있으니 제거해보자\n",
        "min_len = 999\n",
        "max_len = 0\n",
        "sum_len = 0\n",
        "\n",
        "cleaned_corpus = list(set(ratings))  # set를 사용해서 중복을 제거합니다.\n",
        "print(\"Data Size:\", len(cleaned_corpus))\n",
        "\n",
        "for sen in cleaned_corpus:\n",
        "    length = len(sen)\n",
        "    if min_len > length: min_len = length\n",
        "    if max_len < length: max_len = length\n",
        "    sum_len += length\n",
        "\n",
        "print(\"문장의 최단 길이:\", min_len)\n",
        "print(\"문장의 최장 길이:\", max_len)\n",
        "print(\"문장의 평균 길이:\", sum_len // len(cleaned_corpus))\n",
        "\n",
        "sentence_length = np.zeros((max_len), dtype=np.int)\n",
        "\n",
        "for sen in cleaned_corpus:   # 중복이 제거된 코퍼스 기준\n",
        "    sentence_length[len(sen)-1] += 1\n",
        "\n",
        "plt.bar(range(max_len), sentence_length, width=1.0)\n",
        "plt.title(\"Sentence Length Distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O9SZlm5jp8Hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"중복된 값: \",len(ratings) - len(cleaned_corpus))"
      ],
      "metadata": {
        "id": "pkmGdL6lrQvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> - 150,000 개에서 중복된 데이터 3817개를 제거하여 146,183개로 줄었다.\n",
        "\n",
        "1. 네이버 영화 후기의 경우 길이가 1,2 로 짧아도 감정이 드러나기 때문에 min_len 은 1로 하자.\n",
        "2. 후기니까 문장 길이 80이상은 제거하자."
      ],
      "metadata": {
        "id": "LAaA6AITq5JM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 80\n",
        "min_len = 1\n",
        "\n",
        "# 길이 조건에 맞는 문장만 선택합니다.\n",
        "filtered_corpus = [s for s in cleaned_corpus if (len(s) < max_len) & (len(s) >= min_len)]\n",
        "\n",
        "# 분포도를 다시 그려봅니다. -> 최종적인 데이터 분포\n",
        "sentence_length = np.zeros((max_len), dtype=np.int)\n",
        "\n",
        "for sen in filtered_corpus:\n",
        "    sentence_length[len(sen)-1] += 1\n",
        "\n",
        "plt.bar(range(max_len), sentence_length, width=1.0)\n",
        "plt.title(\"Sentence Length Distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4fV7CPysrltj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Konlpy로 형태소 분석해보면?"
      ],
      "metadata": {
        "id": "lFmR1zpsuzE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mecab 설치하기\n",
        "!apt-get update \n",
        "!apt-get install g++ openjdk-8-jdk \n",
        "!pip install konlpy JPype1-py3 \n",
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ],
      "metadata": {
        "id": "6QLMqQlqw-1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  mecab.morphs()를 사용해서 형태소 분석\n",
        "from konlpy.tag import Mecab\n",
        "\n",
        "mecab = Mecab()\n",
        "\n",
        "def mecab_split(sentence):\n",
        "    return mecab.morphs(sentence)\n",
        "\n",
        "mecab_corpus = []\n",
        "\n",
        "for kor in filtered_corpus:\n",
        "    mecab_corpus.append(mecab_split(kor))"
      ],
      "metadata": {
        "id": "MMUhf6RHwd5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# corpus: Tokenized Sentence's List\n",
        "def tokenize(corpus):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)\n",
        "\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "    return tensor, tokenizer"
      ],
      "metadata": {
        "id": "xTsj6KhCyxf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 사전 길이 확인\n",
        "mecab_tensor, mecab_tokenizer = tokenize(mecab_corpus)\n",
        "\n",
        "print(\"MeCab Vocab Size:\", len(mecab_tokenizer.index_word))"
      ],
      "metadata": {
        "id": "X_4jkLKryeTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "mecab 테스트 해보기"
      ],
      "metadata": {
        "id": "_w1kJJiQUicv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Case 1\n",
        "texts = mecab_tokenizer.sequences_to_texts([mecab_tensor[300]])\n",
        "\n",
        "print(texts[0])"
      ],
      "metadata": {
        "id": "h1kAgHV7ylpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Case 2\n",
        "sentence = \"\"\n",
        "\n",
        "for w in mecab_tensor[300]:\n",
        "    if w == 0: continue\n",
        "    sentence += mecab_tokenizer.index_word[w] + \"\"\n",
        "\n",
        "print(sentence)"
      ],
      "metadata": {
        "id": "mFJBei5tO12g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = mecab_tokenizer.texts_to_sequences(\"줄거리에 다써놨어 ㅋㅋㅋ ㅁ ㅣ친 ㄴ ㅔ이버\")\n",
        "test"
      ],
      "metadata": {
        "id": "bdaoEyG8edno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = mecab_tokenizer.sequences_to_texts(test)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "Y02YVDTNfV1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 불용어 제거 어디서 어떻게 하지?"
      ],
      "metadata": {
        "id": "sG_T2bdQBHIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) Sentence Piece를 이용해서 Vocab 파일을 만들기\n"
      ],
      "metadata": {
        "id": "1Qd19xWdO3z3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir"
      ],
      "metadata": {
        "id": "iFjdVOehbQKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_corpus[:10]"
      ],
      "metadata": {
        "id": "n550uFOvTMyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "import os\n",
        "temp_file = data_dir + '/ratings_train_temp4.txt'\n",
        "\n",
        "vocab_size = 10000\n",
        "model_name = 'rating_cor4'\n",
        "\n",
        "with open(temp_file, 'w') as f:\n",
        "    for row in filtered_corpus:   # 이전 스텝에서 정제했던 corpus를 활용합니다.\n",
        "        f.write(str(row) + '\\n')\n",
        "\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    \"--input={} --model_prefix={} --vocab_size={}\".format(temp_file, model_name, vocab_size) +\n",
        "    \" --model_type=char\" + # Train에서  --model_type = 'unigram'이 디폴트, --model_type = 'bpe' 로 옵션을 주어 변경 가능.\n",
        "    \" --max_sentence_length=80\" + # 문장 최대 길이\n",
        "    \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\n",
        "    \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\n",
        "    \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\n",
        "    \" --eos_id=3 --eos_piece=[EOS]\" ) # end of sequence (3)\n",
        "\n",
        "!ls -l rating_cor4*"
      ],
      "metadata": {
        "id": "gdwC9v6SXNUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = spm.SentencePieceProcessor()\n",
        "s.Load('rating_cor4.model')\n",
        "\n",
        "# SentencePiece를 활용한 sentence -> encoding\n",
        "tokensIDs = s.EncodeAsIds('줄거리에 다써놨어 ㅋㅋㅋ ㅁ ㅣ친 ㄴ ㅔ이버')\n",
        "print(tokensIDs)\n",
        "\n",
        "# SentencePiece를 활용한 sentence -> encoded pieces\n",
        "print(s.SampleEncodeAsPieces('줄거리에 다써놨어 ㅋㅋㅋ ㅁ ㅣ친 ㄴ ㅔ이버',1, 0.0))\n",
        "\n",
        "# SentencePiece를 활용한 encoding -> sentence 복원\n",
        "print(s.DecodeIds(tokensIDs))"
      ],
      "metadata": {
        "id": "Qtq_mdjaZUIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. Tokenizer 함수 작정\n",
        "-  위에서 훈련시킨 SentencePiece를 활용\n",
        "> 1. 매개변수로 토큰화된 문장의 list를 전달하는 대신 **온전한 문장**의 list를 전달한다.\n",
        "> 2. **생성된 vocab 파일** 읽어와 `{ <word> : <idx> }` 형태를 가지는 `word_index` 사전과 `{ <idx> : <word>}` 형태를 가지는 `index_word` 사전을 생성하고 함께 **반환**합니다.\n",
        "> 3. 리턴값인 `tensor` 는 앞의 함수와 동일하게 토큰화한 후 Encoding된 문장입니다. 바로 학습에 사용할 수 있게 Padding은 당연히 해야합니다."
      ],
      "metadata": {
        "id": "l-95H7uVb5H2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sp_tokenize(s, corpus):\n",
        "\n",
        "    tensor = []\n",
        "\n",
        "    for sen in corpus:\n",
        "        tensor.append(s.EncodeAsIds(sen))\n",
        "\n",
        "    with open(\"./rating_cor4.vocab\", 'r') as f:\n",
        "        vocab = f.readlines()\n",
        "\n",
        "    word_index = {}\n",
        "    index_word = {}\n",
        "\n",
        "    for idx, line in enumerate(vocab):\n",
        "        word = line.split(\"\\t\")[0]\n",
        "\n",
        "        word_index.update({idx:word})\n",
        "        index_word.update({word:idx})\n",
        "\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='pre') # RNN에는 'pre'가 더 유리하다\n",
        "\n",
        "    return tensor, word_index, index_word"
      ],
      "metadata": {
        "id": "aHkvYArvf_NA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sp_tokenize(s, corpus) 사용예제\n",
        "\n",
        "my_corpus = ['나는 밥을 먹었습니다.', '그러나 여전히 ㅠㅠ 배가 고픕니다...']\n",
        "tensor, word_index, index_word = sp_tokenize(s, my_corpus)\n",
        "print(\"Tensor:\",tensor)\n",
        "print(\"Word_index:\",word_index)\n",
        "print(\"Index_word:\",index_word)"
      ],
      "metadata": {
        "id": "a11dw3_6lqaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 로더 함수 만들기\n",
        "- 데이터 정규표현식 전처리 함수 만들기\n",
        "- 데이터 중복 제거\n",
        "- 데이터 결측치 제거\n",
        "- 불용어 처리\n",
        "- sp_tokenize로 \n"
      ],
      "metadata": {
        "id": "OiYFxZX84NvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> - 한글 불용어 파이썬 라이브러리 없기때문에 직접 불용어 사전 만들어 제거하자!\n",
        "> -  한국어 불용어 리스트 100개 txt 파일 쓰는 방법도 있음!\n",
        "\n",
        "[한국어 불용어 리스트 100개](https://bab2min.tistory.com/544)"
      ],
      "metadata": {
        "id": "AnNfEAVrAyCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# raw 데이터 불러와서 시작하자\n",
        "path_to_file = data_dir + \"/ratings_train.txt\"\n",
        "test_path = data_dir + \"/ratings_test.txt\"\n",
        "\n",
        "train_d = pd.read_table(path_to_file)\n",
        "test_d = pd.read_table(test_path)\n",
        "\n",
        "train_d.head()"
      ],
      "metadata": {
        "id": "3s-64uvQ4Qle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# 불용어 사전 만들기\n",
        "stopwords = set(['하','아','의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다'])\n",
        "\n",
        "def preprocess_data(sentence):\n",
        "    # 한글 및 공백 이외 다른 문자 모두 제거.\n",
        "    sentence = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]\", \"\", sentence)\n",
        "    return sentence\n",
        "\n",
        "def load_data(train_data, test_data, num_words=10000):\n",
        "\n",
        "    # train/test의 중복, 결측치 제거, 한글 및 공백 이외 제거\n",
        "    train_data.drop_duplicates(subset=['document'], inplace=True) \n",
        "    train_data['document'] = train_data['document'].apply(lambda x: preprocess_data(str(x)))\n",
        "    train_data = train_data.dropna(how = 'any')\n",
        "\n",
        "    test_data.drop_duplicates(subset=['document'], inplace=True)\n",
        "    test_data['document'] = test_data['document'].apply(lambda x: preprocess_data(str(x)))\n",
        "    test_data = test_data.dropna(how = 'any') \n",
        "\n",
        "    train_list = []\n",
        "    for sentence in train_data['document']:\n",
        "        # temp_X = s.encode_as_pieces(sentence) # 토큰화\n",
        "        # temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
        "        # temp_X = s.encode_as_ids(sentence)\n",
        "        train_list.append(sentence)\n",
        "\n",
        "    test_list = []\n",
        "    for sentence in test_data['document']:\n",
        "        # temp_X = s.encode_as_pieces(sentence) # 토큰화\n",
        "        # temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
        "        # temp_X = s.encode_as_ids(sentence)\n",
        "        test_list.append(sentence)\n",
        "    \n",
        "    X_train, word_to_index, index_to_word = sp_tokenize(s, train_list)\n",
        "    X_test , _,_ = sp_tokenize(s, test_list)\n",
        "\n",
        "        \n",
        "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n",
        "    \n",
        "X_train, y_train, X_test, y_test, word_to_index = load_data(train_d, test_d) "
      ],
      "metadata": {
        "id": "4CoV9dWJAUtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "5aPDYNRU3G5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "BTPWKtw03K-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "id": "ASMRf_m33MEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "id": "ftI_929V3NNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word_to_index"
      ],
      "metadata": {
        "id": "n5HY-Phi3O3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4. 모델 구성하기\n",
        "- 1. LSTM \n",
        "- 2. 1-D CNN\n",
        "- 3. GlobalMaxPooling1D() 레이어 하나만 쓰기\n",
        "- 4. Transformer 이용 (시간 더 있을 시)"
      ],
      "metadata": {
        "id": "cQUdO-3v2Ahk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM 모델 구성**"
      ],
      "metadata": {
        "id": "YN23fFR75jUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM 모델\n",
        "vocab_size = 8000\n",
        "word_vector_dim = 32 \n",
        "\n",
        "model_lstm = tf.keras.Sequential()\n",
        "model_lstm.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
        "model_lstm.add(tf.keras.layers.LSTM(64))\n",
        "model_lstm.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "model_lstm.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
        "\n",
        "model_lstm.summary()"
      ],
      "metadata": {
        "id": "1ot7xx4j3fGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1-D CNN 모델 구성**"
      ],
      "metadata": {
        "id": "FNRV_aHN_msn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1-D CNN으로 만든 모델 구현\n",
        "\n",
        "model_1dc = tf.keras.Sequential()\n",
        "model_1dc.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
        "model_1dc.add(tf.keras.layers.Conv1D(32, 7, activation='relu')) # word_vector_dim과 맞춰봤습니다.\n",
        "model_1dc.add(tf.keras.layers.MaxPooling1D(5))\n",
        "model_1dc.add(tf.keras.layers.Conv1D(32, 7, activation='relu')) # word_vector_dim과 맞춰봤습니다.\n",
        "model_1dc.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "model_1dc.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model_1dc.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
        "\n",
        "model_1dc.summary()"
      ],
      "metadata": {
        "id": "eQp6GHeE_tof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GlobalMaxPooling1D() 레이어만 사용한 모델**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UD1PTfyp_L0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GlobalMaxPolling만 있는 것 \n",
        "\n",
        "model_gmp = tf.keras.Sequential()\n",
        "model_gmp.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
        "model_gmp.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "model_gmp.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model_gmp.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
        "\n",
        "model_gmp.summary()"
      ],
      "metadata": {
        "id": "AvCmSLwg_Rxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validations set 구분**"
      ],
      "metadata": {
        "id": "5iwUIAFS_KpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_test_split으로 나눠보겠습니다.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "xx_train, xx_validation, yy_train, yy_validation = train_test_split(X_train,\n",
        "                                                                   y_train,\n",
        "                                                                   test_size=0.2,\n",
        "                                                                   random_state=2022)"
      ],
      "metadata": {
        "id": "AObU72kjAUcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 에포크\n",
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "BQmRFa89-E4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. LSTM\n"
      ],
      "metadata": {
        "id": "U4wpI8Ln9vd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model_lstm.fit(xx_train,\n",
        "                         yy_train,\n",
        "                         epochs=EPOCHS,\n",
        "                         batch_size=512,\n",
        "                        validation_data=(xx_validation, yy_validation),\n",
        "                        verbose=2)"
      ],
      "metadata": {
        "id": "PKWwoRUS9Gxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test 셋으로 평가해보기\n",
        "results = model_lstm.evaluate(X_test, y_test, verbose=2)\n",
        "\n",
        "print(results)"
      ],
      "metadata": {
        "id": "huMTjQJIExho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 그래프로 그려보기\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "loss = history.history['loss']\n",
        "val_acc = history.history['val_accuracy']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(acc, label='training')\n",
        "plt.plot(val_acc, 'o', label='validation')\n",
        "plt.xlabel('epocks')\n",
        "plt.ylabel('accuracy', fontsize=20)\n",
        "plt.legend(fontsize=20)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(loss, label='training')\n",
        "plt.plot(val_loss, 'o', label='validation')\n",
        "plt.xlabel('epocks')\n",
        "plt.ylabel('loss', fontsize=20)\n",
        "plt.legend(fontsize=20)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "opnSh1ApE_0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 1-D CNN"
      ],
      "metadata": {
        "id": "7F9iQtSVFHVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_1dc.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model_1dc.fit(xx_train,\n",
        "                         yy_train,\n",
        "                         epochs=EPOCHS,\n",
        "                         batch_size=512,\n",
        "                        validation_data=(xx_validation, yy_validation),\n",
        "                        verbose=2)"
      ],
      "metadata": {
        "id": "axdQoEEzFPhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test 셋으로 평가해보기\n",
        "results = model_1dc.evaluate(X_test, y_test, verbose=2)\n",
        "\n",
        "print(results)"
      ],
      "metadata": {
        "id": "y5GG-_LBFX-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 그래프로 그려보기\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "loss = history.history['loss']\n",
        "val_acc = history.history['val_accuracy']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(acc, label='training')\n",
        "plt.plot(val_acc, 'o', label='validation')\n",
        "plt.xlabel('epocks')\n",
        "plt.ylabel('accuracy', fontsize=20)\n",
        "plt.legend(fontsize=20)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(loss, label='training')\n",
        "plt.plot(val_loss, 'o', label='validation')\n",
        "plt.xlabel('epocks')\n",
        "plt.ylabel('loss', fontsize=20)\n",
        "plt.legend(fontsize=20)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xb0IHHmXFoiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Global Max Pooling"
      ],
      "metadata": {
        "id": "q8NBTeZcFtSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_gmp.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model_gmp.fit(xx_train,\n",
        "                         yy_train,\n",
        "                         epochs=EPOCHS,\n",
        "                         batch_size=512,\n",
        "                        validation_data=(xx_validation, yy_validation),\n",
        "                        verbose=2)"
      ],
      "metadata": {
        "id": "1e9L1s0DFwpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test 셋으로 평가해보기\n",
        "results = model_gmp.evaluate(X_test, y_test, verbose=2)\n",
        "\n",
        "print(results)"
      ],
      "metadata": {
        "id": "M19kx919F0Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 그래프로 그려보기\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "loss = history.history['loss']\n",
        "val_acc = history.history['val_accuracy']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(acc, label='training')\n",
        "plt.plot(val_acc, 'o', label='validation')\n",
        "plt.xlabel('epocks')\n",
        "plt.ylabel('accuracy', fontsize=20)\n",
        "plt.legend(fontsize=20)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(loss, label='training')\n",
        "plt.plot(val_loss, 'o', label='validation')\n",
        "plt.xlabel('epocks')\n",
        "plt.ylabel('loss', fontsize=20)\n",
        "plt.legend(fontsize=20)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rnoo6H5KF5tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 실험 결과\n",
        "1. vocab size = 8000, model_type = bpe 일 때\n",
        "\n",
        "> LSTM \n",
        "```\n",
        "1537/1537 - 6s - loss: 0.4196 - accuracy: 0.8348 - 6s/epoch - 4ms/step\n",
        "[0.41956353187561035, 0.8348386883735657]\n",
        "```\n",
        "\n",
        "> 1-D CNN\n",
        "```\n",
        "1537/1537 - 4s - loss: 1.1662 - accuracy: 0.7817 - 4s/epoch - 3ms/step\n",
        "[1.1662278175354004, 0.7817038893699646]\n",
        "```\n",
        "\n",
        "> Global Max Pooling\n",
        "```\n",
        "1537/1537 - 3s - loss: 0.4817 - accuracy: 0.8334 - 3s/epoch - 2ms/step\n",
        "[0.4816548824310303, 0.8333536982536316]\n",
        "```"
      ],
      "metadata": {
        "id": "YdSmTtlnGDDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. vocab size = 10000, model_type = bpe 일 때\n",
        "\n",
        "> LSTM \n",
        "```\n",
        "1537/1537 - 7s - loss: 0.4194 - accuracy: 0.8371 - 7s/epoch - 5ms/step\n",
        "[0.4194413721561432, 0.8370763659477234]\n",
        "```\n",
        "\n",
        "> 1-D CNN\n",
        "```\n",
        "1537/1537 - 5s - loss: 1.1183 - accuracy: 0.7455 - 5s/epoch - 3ms/step\n",
        "[1.1182972192764282, 0.745514452457428]\n",
        "```\n",
        "\n",
        "> Global Max Pooling\n",
        "```\n",
        "1537/1537 - 4s - loss: 0.4168 - accuracy: 0.8358 - 4s/epoch - 3ms/step\n",
        "[0.4167833626270294, 0.8357744216918945]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "FlIelozpQG2M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. vocab size = 10000, model_type = unigram(default) 일 때\n",
        "\n",
        "> LSTM \n",
        "```\n",
        "1537/1537 - 7s - loss: 0.3842 - accuracy: 0.8381 - 7s/epoch - 5ms/step\n",
        "[0.3842264413833618, 0.8380731344223022]\n",
        "```\n",
        "\n",
        "> 1-D CNN\n",
        "```\n",
        "1537/1537 - 5s - loss: 1.1931 - accuracy: 0.7818 - 5s/epoch - 3ms/step\n",
        "[1.1930625438690186, 0.781846284866333]\n",
        "```\n",
        "\n",
        "> Global Max Pooling\n",
        "```\n",
        "1537/1537 - 4s - loss: 0.4721 - accuracy: 0.8319 - 4s/epoch - 3ms/step\n",
        "[0.4721464216709137, 0.8319296836853027]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "yuZ8_nkVfxhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. vocab size = 10000, model_type = char 일 때\n",
        "\n",
        "> LSTM \n",
        "```\n",
        "1537/1537 - 7s - loss: 0.3872 - accuracy: 0.8224 - 7s/epoch - 4ms/step\n",
        "[0.3871708810329437, 0.8224093914031982]\n",
        "```\n",
        "\n",
        "> 1-D CNN -> 오버피팅 발생!!!\n",
        "```\n",
        "1537/1537 - 4s - loss: 0.4319 - accuracy: 0.8254 - 4s/epoch - 3ms/step\n",
        "[0.43188703060150146, 0.8253793716430664]\n",
        "```\n",
        "\n",
        "> Global Max Pooling\n",
        "```\n",
        "1537/1537 - 3s - loss: 0.4471 - accuracy: 0.7967 - 3s/epoch - 2ms/step\n",
        "[0.4470667541027069, 0.7966556549072266]\n",
        "```"
      ],
      "metadata": {
        "id": "g4R3LN-ehgfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**=> 형태소 분석기를 썼을 때와 정확도 차이가 크게 안난다.**"
      ],
      "metadata": {
        "id": "5NnBGM6IpeEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 회고\n",
        "---\n",
        "> - 처음에 데이터 불러오는 방법이 여러가지고 코드가 익숙하지 않아 많이 헤맸다. 이제 이런 일 없도록 해야겠다.\n",
        "> - NLP 텍스트 전처리 과정이 명확하게 머리에 안그려지는 상황에서 SentenciePiece를 활용하려고 하다보니 데이터 로더 함수를 짜는데도 시간이 좀 소요되었다. 텍스트 전처리할때 토크나이저 패키지를 잘 활용할 줄 아는 것도 효율적으로 시간을 쓸 수 있는 방법이라는 것을 깨달았다.\n",
        "> - Korpora 라는 오픈소스 패키지를 발겼했다. 한국어 텍스트 데이터를 바로 불러와서 쓸 수 있는 유용한 패키지 같은데 다음에 한번 활용해봐야겠다.\n",
        "> - **근데 아직 SentencePiece로 만든 토크나이저 함수를 활용할 때 불용어 제거를 어느 부분에서 해야할 지 모르겠다. 이부분은 자연어처리 고수분에게 물어보고 넘어가자.**\n",
        "> - 불용어 제거할 때, 한국어는 불용어 제거 패키지가 없어 직접 불용어 사전을 만들어야 한다고 했다. 하지만 한국어 불용어 모음 텍스트 파일을 공유하기도 하니 불용어 사전을 만들 때 이것을 가져와 쓰거나 아니면 필요한 부분만 가져와서 사용하는 것도 좋을 것 같다.\n",
        "> - 단어장 만들 때 vocab_size 와 model_type을 바꿔가면서 정확도가 어떻게 달라지는 지 실험을 했는데 LSTM & Global Max Pooling 과 1-D CNN이 반비례 처럼 달라졌다. 같은 모델 타입(bpe)에서 vocab_size를 더 크게 잡았을 땐, LSTM 과 GMP는 정확도가 조금 증가하였지만 1-D CNN은 오히려 더 떨어졌다. 그리고 마지막에 model_type default인 unigram으로 단어장을 만들어 학습시켰을 때는 LSTM, GMP는 정확도가 떨어졌지만 1-D CNN은 정확도가 80%가 넘으면 많이 올랐다.\n",
        "> - 단순한 모델을 사용했기 때문에 다음엔 더 공부해서 Transformer와 BERT도 사용하고 워드 벡터 차원, 에포크 수도 늘려서 학습시켜볼 예정이다.\n",
        "> - 그때는 오버피팅도 해결하면서 학습시켜보는걸로!\n",
        "\n",
        "> **다음에 할 것을 정리해보자면**\n",
        "> 1. 단어장을 네이버 영화 리뷰 말고 더 큰 한국어 텍스트로 만들어보기\n",
        "> 2. 불용어 제거하는 방법 찾아서 불용어 제거하고 Sentencepiece 토크나이저 돌리기\n",
        "> 3. 좀 더 복잡한 모델 transformer로 돌리기\n",
        "> 4. 오버피팅나면 정규화해서 학습시키기"
      ],
      "metadata": {
        "id": "gWBrx-4Gi6th"
      }
    }
  ]
}